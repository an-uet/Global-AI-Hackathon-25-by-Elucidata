{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":94147,"databundleVersionId":11390004,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport h5py\nimport numpy as np\nfrom scipy.stats import spearmanr\nfrom sklearn.model_selection import train_test_split\n\nimport h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport wandb\nfrom scipy.stats import spearmanr\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import train_test_split\nfrom collections import defaultdict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom scipy.stats import spearmanr\nimport cv2\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA And preprocess data","metadata":{}},{"cell_type":"code","source":"import h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef plot_celltype_abundance(slide_name, image, spots, plots_per_line=4, show_image=False):\n    \"\"\"\n    Plot the slide image with spot overlays for cell-type abundances (C1 to C35).\n\n    Parameters:\n        slide_name (str): Name of the slide.\n        image (np.array): 2D or 3D array representing the slide image.\n        spots (np.array): Structured NumPy array containing spot information with fields:\n                          'x', 'y', 'C1', 'C2', ..., 'C35'.\n        plots_per_line (int): Number of subplots per row (default is 4).\n\n    The function creates a figure with subplots arranged in a grid and plots:\n      - The slide image as a background.\n      - A scatter overlay at spot (x, y) positions, colored by the abundance of each cell type.\n      - A common colorbar that indicates the mapping from color to abundance.\n    \"\"\"\n    # Extract the x and y coordinates from the spots array\n    x = spots[\"x\"]\n    y = spots[\"y\"]\n    num_celltypes = 35  # There are 35 cell types (C1 to C35)\n    num_rows = math.ceil(num_celltypes / plots_per_line)\n    \n    # Create a figure with a grid of subplots\n    fig, axes = plt.subplots(num_rows, plots_per_line, figsize=(plots_per_line * 3, num_rows * 3))\n    axes = axes.flatten()  # Flatten to simplify indexing\n    \n    # Loop over each cell type field from C1 to C35\n    for i in range(num_celltypes):\n        var_name = f\"C{i+1}\"  # Create field name e.g. \"C1\", \"C2\", ...\n        c_values = spots[var_name]  # Get the abundance values for this cell type\n        \n        ax = axes[i]\n        if show_image is True:\n            ax.imshow(image, aspect=\"auto\")\n        else:\n            # Manually set the axis limits to match the image dimensions\n            height, width = image.shape[:2]\n            ax.set_xlim(0, width)\n            ax.set_ylim(height, 0)  # invert y-axis to mimic image display\n\n        sc = ax.scatter(x, y, c=c_values, cmap=\"viridis\", s=2, alpha=1)\n        #sc = ax.scatter(x, y, c=c_values, cmap=\"plasma\", s=2, alpha=0.7)\n\n        ax.set_title(var_name, fontsize=8)\n        ax.axis(\"off\")\n    \n    # If there are any extra subplots (in case grid has one more cell), hide them\n    for j in range(num_celltypes, len(axes)):\n        axes[j].axis(\"off\")\n    \n    # Set an overall title for the figure\n    fig.suptitle(f\"Slide {slide_name}\", fontsize=14)\n    \n    # Add a common colorbar (using the last scatter object)\n    #fig.colorbar(sc, ax=axes.tolist(), label=\"Abundance\")\n    #fig.subplots_adjust(right=0.85)\n    #cbar_ax = fig.add_axes([0.88, 0.15, 0.03, 0.7])  # [left, bottom, width, height]\n    #fig.colorbar(sc, cax=cbar_ax, label=\"Abundance\")\n    \n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    plt.show()\n\n\ndef plot_total_abundance(slide_name, image, spots, show_colorbar=True, colorbar_outside=True, show_image=True, shift_x = 0, shift_y = 0 ):\n    \"\"\"\n    Plot the slide image with spot overlays where each spot's color represents \n    the total abundance of cell types C1 to C35.\n\n    Parameters:\n        slide_name (str): Name of the slide.\n        image (np.array): The slide image.\n        spots (np.array): Structured NumPy array with fields 'x', 'y', and 'C1' ... 'C35'.\n        show_colorbar (bool): Whether to display a colorbar (default True).\n        colorbar_outside (bool): If True, place the colorbar to the right of the plot.\n    \"\"\"\n    # Extract x and y coordinates from the spots array\n    x = spots[\"x\"] +  shift_x\n    y = spots[\"y\"] +  shift_y\n    \n    # Compute total abundance by summing C1 through C35 for each spot\n    total_abundance = np.zeros_like(x, dtype=float)\n    if slide_name == \"S_7\":\n        total_abundance = spots[\"Test_Set\"]\n    else:\n        for i in range(1, 36):  # Fields C1 to C35\n            total_abundance += spots[f\"C{i}\"]\n    \n    # Create a figure\n    fig, ax = plt.subplots(figsize=(6, 6))\n    if show_image is True:\n        ax.imshow(image, aspect=\"auto\")\n    else:\n        # Manually set the axis limits to match the image dimensions\n        height, width = image.shape[:2]\n        ax.set_xlim(0, width)\n        ax.set_ylim(height, 0)  # invert y-axis to mimic image display\n\n    sc = ax.scatter(x, y, c=total_abundance, cmap=\"viridis\", s=2, alpha=0.7)\n    ax.set_title(f\"Total Abundance for Slide {slide_name}\")\n    ax.axis(\"off\")\n    \n    # Add a colorbar if desired\n    if show_colorbar:\n        if colorbar_outside:\n            # Adjust the right margin to make room for the colorbar\n            fig.subplots_adjust(right=0.85)\n            cbar_ax = fig.add_axes([0.88, 0.15, 0.03, 0.7])\n            fig.colorbar(sc, cax=cbar_ax, label=\"Total Abundance\")\n        else:\n            fig.colorbar(sc, ax=ax, label=\"Total Abundance\")\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# S1: x - 60, y - 60\n# S2: x - 70, y - 70\n# S3: x - 20, y - 20\n# S4: x - 10, y - 10\n# S5: x - 10, y - 10\n# S6:  x - 10, y - 10","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"slice_name = f\"S_6\"\nwith h5py.File(\"/kaggle/input/el-hackathon-2025/elucidata_ai_challenge_data.h5\", \"r\") as h5file:\n    image = np.array(h5file[\"images/Train\"][slice_name])\n    print(image.shape)\n    spots = np.array(h5file[\"spots/Train\"][slice_name])\n    plot_total_abundance(slice_name, image, spots, show_colorbar=True, colorbar_outside=True,  shift_x = -10, shift_y = -10)\n    plot_total_abundance(slice_name, image, spots, show_colorbar=True, colorbar_outside=True, show_image=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config_shilf = {\n    \"S_1\": 60,\n    \"S_2\": 70,\n    \"S_3\": 20,\n    \"S_4\": 10,\n    \"S_5\": 10,\n    \"S_6\": 10,\n    \"S_7\": 0,\n\n}\nclass CellDataset(data.Dataset):\n    def __init__(self, mode, images=None, coords=None, labels=None, transform=None):\n        self.images = images if images is not None else []\n        self.coords = coords if coords is not None else []\n        self.labels = labels if labels is not None else []\n        self.transform = transform  # Add transform support\n\n        if mode in [\"Train\", \"Test\"] and images is None:\n            with h5py.File(\"/kaggle/input/el-hackathon-2025/elucidata_ai_challenge_data.h5\", \"r\") as h5file:\n                images_group = h5file[f\"images/{mode}\"]\n                spots_group = h5file[f\"spots/{mode}\"]\n\n                for slide_name in images_group.keys():\n                    image = np.array(images_group[slide_name])\n                    spots = np.array(spots_group[slide_name])\n                    spots = pd.DataFrame(spots)\n                    subimage_size = 110\n                    half_size = subimage_size // 2\n\n                    for i in range(spots.shape[0]):\n                        x1, y1 = int(spots[\"x\"][i]), int(spots[\"y\"][i])\n                        x = int(x1 - config_shilf[slide_name])\n                        y = int(y1 - config_shilf[slide_name])\n                        cell_values = spots.iloc[i][2:].values.tolist()\n\n                        x_min = max(0, x - half_size)\n                        x_max = min(image.shape[1], x + half_size)\n                        y_min = max(0, y - half_size)\n                        y_max = min(image.shape[0], y + half_size)\n\n                        subimage = image[y_min:y_max, x_min:x_max]\n                        subimage = (subimage * 255).astype(np.uint8)  # Normalize and convert to uint8\n                        # cv2.imwrite(f'/content/sample_data/data/{slide_name}_{x1}_{y1}.jpg', subimage)\n\n                        if subimage.shape[:2] == (subimage_size, subimage_size):\n                            self.images.append(subimage)\n                            self.coords.append((x, y))\n                            self.labels.append(cell_values)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = torch.tensor(self.images[idx], dtype=torch.float32).permute(2, 0, 1) / 255.0\n        coords = torch.tensor(self.coords[idx], dtype=torch.float32)\n        labels = torch.tensor(self.labels[idx], dtype=torch.float32)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, coords, labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 0.3\nclass CellProportionPredictor(nn.Module):\n    def __init__(self, num_cell_types):\n        super().__init__()\n        self.cnn = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\n        self.cnn.fc = nn.Identity()  # Remove final layer\n\n        # Coordinate processing\n        self.coord_fc = nn.Sequential(\n            nn.Linear(2, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128)\n        )\n\n        self.head = nn.Sequential(\n            nn.Linear(2048 + 128, 512),  \n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, num_cell_types),\n            nn.ReLU()\n        )\n\n    def forward(self, image, coords):\n        features = self.cnn(image)\n        coord_features = self.coord_fc(coords)\n        combined = torch.cat([features, coord_features], dim=1)\n        return self.head(combined)\n\n\nclass KLDivLoss(nn.Module):\n    def forward(self, pred, target):\n        pred = torch.clamp(pred, 1e-7, 1.0)\n        target = torch.clamp(target, 1e-7, 1.0)\n        return (target * (torch.log(target) - torch.log(pred))).sum(dim=1).mean()\n\n# Metrics\ndef metric_monitor(pred, target):\n    metrics = {\n        'Cosine': nn.CosineSimilarity()(pred, target).mean(),\n        'Spearman': spearmanr(pred.cpu().detach(),\n                              target.cpu().detach()).correlation\n    }\n    return metrics\n\n\nclass CombinedLoss(nn.Module):\n    def __init__(self, alpha=0.5):\n        \"\"\"\n        Combined loss using KL Divergence and Spearman correlation.\n        :param alpha: Weight for KL loss, (1-alpha) is for Spearman loss.\n        \"\"\"\n        super().__init__()\n        self.alpha = alpha\n        self.kl_loss = KLDivLoss()  # Use your defined KLDivLoss\n\n    def spearman_loss(self, pred, target):\n        \"\"\"\n        Approximate Spearman rank correlation as a loss.\n        \"\"\"\n        pred_rank = torch.argsort(torch.argsort(pred, dim=1), dim=1).float()\n        target_rank = torch.argsort(torch.argsort(target, dim=1), dim=1).float()\n\n        pred_rank = pred_rank / (pred.shape[1] - 1)\n        target_rank = target_rank / (target.shape[1] - 1)\n\n        spearman_corr = F.cosine_similarity(pred_rank, target_rank, dim=1)\n        return 1 - spearman_corr.mean() \n\n    def forward(self, pred, target):\n        \"\"\"\n        Compute the combined loss.\n        \"\"\"\n        kl = self.kl_loss(pred, target)\n        spearman = self.spearman_loss(pred, target)\n        return self.alpha * kl + (1 - self.alpha) * spearman\n\nfrom scipy.stats import spearmanr\nimport numpy as np\n\ndef compute_spearman(y_pred, y_true):\n    correlations = []\n    y_pred = y_pred.cpu().numpy()\n    y_true = y_true.cpu().numpy()\n\n    for i in range(y_pred.shape[0]):  \n        corr, _ = spearmanr(y_pred[i], y_true[i])\n        if not np.isnan(corr):  \n            correlations.append(corr)\n\n    return np.mean(correlations) if correlations else 0.0  \n\n\ndef train_model():\n    best_val_loss = float(\"inf\")\n    best_model_path = \"best_model.pth\"\n    checkpoint_path = \"checkpoint.pth\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    full_train_set = CellDataset(\"Train\")\n\n    full_val_set = CellDataset(\"Test\")\n\n    train_loader = DataLoader(\n        full_train_set, batch_size=64, shuffle=True, num_workers=4,\n        pin_memory=True, persistent_workers=(4 > 0))\n\n    val_loader = DataLoader(\n        full_val_set, batch_size=64, shuffle=False, num_workers=4,\n        pin_memory=True)\n\n    model = CellProportionPredictor(num_cell_types=35).to(device)\n    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n    # criterion = KLDivLoss()\n    criterion = CombinedLoss()\n\n    scaler = torch.cuda.amp.GradScaler()\n\n    for epoch in range(1000):\n        model.train()\n        total_train_loss = 0.0\n\n        for images, coords, labels in train_loader:\n            images, coords, labels = images.to(device), coords.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            with torch.cuda.amp.autocast():\n                outputs = model(images, coords)\n                loss = criterion(outputs, labels)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            total_train_loss += loss.item()\n\n        avg_train_loss = total_train_loss / len(train_loader)\n        \n        model.eval()\n        total_val_loss = 0.0\n        metrics = defaultdict(float)\n        total_spearman = 0.0\n        num_batches = 0\n\n        with torch.no_grad():\n            for images, coords, labels in train_loader:\n                images, coords, labels = images.to(device), coords.to(device), labels.to(device)\n\n                outputs = model(images, coords)\n                loss = criterion(outputs, labels)\n                total_val_loss += loss.item()\n\n                # Compute batch-wise Spearman correlation\n                batch_spearman = compute_spearman(outputs, labels)\n                total_spearman += batch_spearman\n\n        avg_spearman = total_spearman / len(train_loader)\n        scheduler.step(1-avg_spearman)\n\n        print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, train spearman = {avg_spearman:.4f} \")\n        # wandb.log({\n        #     \"epoch\": epoch + 1,\n        #     \"train_loss\": avg_train_loss,\n        #     \"val_loss\": avg_val_loss,\n        #     \"Spearman\" : metrics[\"Spearman\"] / len(test_loader),\n        #     \"train_spearman\" : avg_spearman\n        # })\n\n\n\n        # **Save checkpoint (for continuing training)**\n        checkpoint = {\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'scaler_state_dict': scaler.state_dict(),\n            'best_val_loss': best_val_loss\n        }\n\n        torch.save(checkpoint, checkpoint_path)\n\n   \n        predictions = []\n\n        with torch.no_grad():\n                for images, coords, labels in val_loader:\n                    images, coords, labels = images.to(device), coords.to(device), labels.to(device)\n\n                    outputs = model(images, coords)\n                    predictions.extend(outputs.cpu().detach().numpy())\n                    # Save predictions\n                columns = [f\"C{k}\" for k in range(1, 36)]\n                df_predictions = pd.DataFrame(predictions, columns=columns)\n                df_predictions['ID'] = df_predictions.index\n                df_predictions.to_csv(\"predictions.csv\", index=False)\n                # wandb.save(\"predictions.csv\")\n\n    print(\"Training complete.\")\n\n\ntrain_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ViT-Based Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision.models import vit_b_16\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclass SpotViT(nn.Module):\n    def __init__(self, num_classes=35):\n        super(SpotViT, self).__init__()\n        \n        # Load pre-trained ViT\n        self.vit = vit_b_16(pretrained=True)\n\n        # Modify classifier head\n        in_features = self.vit.heads.head.in_features\n        self.vit.heads = nn.Identity()  # Remove the original classifier\n\n        # MLP head for cell-type prediction\n        self.fc = nn.Sequential(\n            nn.Linear(in_features, 512),\n            nn.ReLU(),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        features = self.vit(x)  # Extract features from ViT\n        out = self.fc(features)  # Predict 35 cell-type values\n        return out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.model_selection import train_test_split\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Data augmentation\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n])\n\n# Load full dataset\nfull_train_set = CellDataset(\"Train\", transform=train_transform)\nfull_val_set = CellDataset(\"Test\", transform=train_transform)\n\n# Split dataset into training and validation\ntrain_indices, test_indices = train_test_split(\n    range(len(full_train_set)), test_size=0.05, random_state=42\n)\n\ntrain_set = Subset(full_train_set, train_indices)\ntest_set = Subset(full_train_set, test_indices)\n\n# Create data loaders\ntrain_loader = DataLoader(\n    train_set, batch_size=64, shuffle=True, num_workers=4,\n    pin_memory=True, persistent_workers=(4 > 0)\n)\ntest_loader = DataLoader(\n    test_set, batch_size=64, shuffle=False, num_workers=4,\n    pin_memory=True\n)\nval_loader = DataLoader(\n    full_val_set, batch_size=64, shuffle=False, num_workers=4,\n    pin_memory=True\n)\n\n# Initialize Model\nmodel = SpotViT().to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n\n# Training loop\nnum_epochs = 1000\nbest_val_loss = float(\"inf\")\nbest_model_path = \"best_model.pth\"\ncheckpoint_path = \"checkpoint.pth\"\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0  # Reset loss for each epoch\n\n    for images, coords, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    avg_train_loss = running_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}\")\n\n    # **Validation Phase**\n    model.eval()\n    val_loss = 0.0\n    predictions = []\n    with torch.no_grad():\n        for images, coords, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            predictions.extend(outputs.cpu().detach().numpy())\n\n    avg_val_loss = val_loss / len(val_loader)\n    print(f\"Epoch {epoch+1}: Validation Loss={avg_val_loss:.4f}\")\n\n    # Save checkpoint\n    checkpoint = {\n        'epoch': epoch + 1,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }\n    torch.save(checkpoint, checkpoint_path)\n\n    # Save best model based on validation loss\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        torch.save(checkpoint, best_model_path)\n        print(f\"New best model saved with val loss: {best_val_loss:.4f}\")\n\n        # Save predictions\n        columns = [f\"C{k}\" for k in range(1, 36)]\n        df_predictions = pd.DataFrame(predictions, columns=columns)\n        df_predictions['ID'] = df_predictions.index\n        df_predictions.to_csv(\"predictions.csv\", index=False)\n\nprint(\"Training complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Graph","metadata":{}},{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch_geometric\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GCNConv, GATConv, GraphSAGE\nimport numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import kneighbors_graph\n\n\nclass CellDataset(data.Dataset):\n    def __init__(self, mode, images=None, coords=None, labels=None, slide_ids=None, transform=None):\n        self.images = images if images is not None else []\n        self.coords = coords if coords is not None else []\n        self.labels = labels if labels is not None else []\n        self.slide_ids = slide_ids if slide_ids is not None else []  # Add slide ID\n        self.transform = transform  \n\n        if mode in [\"Train\", \"Test\"] and images is None:\n            with h5py.File(\"/kaggle/input/el-hackathon-2025/elucidata_ai_challenge_data.h5\", \"r\") as h5file:\n                images_group = h5file[f\"images/{mode}\"]\n                spots_group = h5file[f\"spots/{mode}\"]\n\n                for slide_id, slide_name in enumerate(images_group.keys()):  # Unique slide_id\n                    image = np.array(images_group[slide_name])\n                    spots = np.array(spots_group[slide_name])\n                    spots = pd.DataFrame(spots)\n\n                    for i in range(spots.shape[0]):\n                        x, y = int(spots[\"x\"][i]), int(spots[\"y\"][i])\n                        cell_values = spots.iloc[i][2:].values.tolist()\n                        subimage_size = 224\n                        half_size = subimage_size // 2\n                        x1, y1 = int(spots[\"x\"][i]), int(spots[\"y\"][i])\n                        x = int(x1 - config_shilf[slide_name])\n                        y = int(y1 - config_shilf[slide_name])\n                        cell_values = spots.iloc[i][2:].values.tolist()\n\n                        x_min = max(0, x - half_size)\n                        x_max = min(image.shape[1], x + half_size)\n                        y_min = max(0, y - half_size)\n                        y_max = min(image.shape[0], y + half_size)\n\n                        subimage = image[y_min:y_max, x_min:x_max]\n                        subimage = (subimage * 255).astype(np.uint8)\n\n                        self.images.append(subimage)\n                        self.coords.append((x, y))\n                        self.labels.append(cell_values)\n                        self.slide_ids.append(slide_id)  \n\n    def __getitem__(self, idx):\n        image = torch.tensor(self.images[idx], dtype=torch.float32).permute(2, 0, 1) / 255.0\n        coords = torch.tensor(self.coords[idx], dtype=torch.float32)\n        labels = torch.tensor(self.labels[idx], dtype=torch.float32)\n        slide_id = torch.tensor(self.slide_ids[idx], dtype=torch.int64)\n\n        return image, coords, labels, slide_id\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport timm\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom sklearn.neighbors import kneighbors_graph\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom tqdm import tqdm\n\n\nvit = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\nvit.head = torch.nn.Linear(768, 512)  # Reduce feature size to 512\nvit.eval()\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_features(image):\n    image = (image * 255).astype(np.uint8) \n    image = Image.fromarray(image).convert(\"RGB\")  \n    image = transform(image).unsqueeze(0)  \n    with torch.no_grad():\n        features = vit(image)  \n    return features.squeeze()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = CellDataset(mode='Train')\n\nimages = list(data.images)\ncoords = np.array(data.coords)\nlabels = np.array(data.labels, dtype=object) \nslide_ids = np.array(data.slide_ids)\n\n\nedge_index_list = []\n\nunique_wsi_ids = np.unique(slide_ids)\nfor wsi_id in unique_wsi_ids:\n    mask = (slide_ids == wsi_id)\n    coords_wsi = coords[mask]\n    node_indices = np.where(mask)[0]\n\n    k = 10\n    adj_matrix = kneighbors_graph(coords_wsi, k, mode=\"connectivity\", include_self=False)\n\n    edge_list = np.array(adj_matrix.nonzero())\n    edge_list[0] = node_indices[edge_list[0]]\n    edge_list[1] = node_indices[edge_list[1]]\n\n    edge_index_list.append(torch.tensor(edge_list, dtype=torch.long))\n\n# Merge all edges\nedge_index = torch.cat(edge_index_list, dim=1)\n\n# Convert images to feature vectors (flatten each image)\n# x = torch.stack([torch.tensor(img, dtype=torch.float).flatten() for img in images])\n\nfeatures = []\nfor img in tqdm(images, desc=\"Extracting features\", total=len(images)):\n    features.append(extract_features(img))\n\nx = torch.stack(features)\n\nlabels = [np.array(label, dtype=np.float32) for label in labels]  \ny = torch.tensor(np.vstack(labels), dtype=torch.float32)\n\ngraph_data = Data(x=x, edge_index=edge_index, y=y)\n\nprint(f\"Graph created with {graph_data.num_nodes} nodes and {graph_data.num_edges} edges.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datatest = CellDataset(mode='Test')\n# Extract data\nimages_test = list(datatest.images) \ncoords = np.array(datatest.coords)\nlabels = np.array(datatest.labels, dtype=object)  \nslide_ids = np.array(datatest.slide_ids)\n\n# Build Graph\nedge_index_list = []\n\nunique_wsi_ids = np.unique(slide_ids)\nfor wsi_id in unique_wsi_ids:\n    mask = (slide_ids == wsi_id)\n    coords_wsi = coords[mask]\n    node_indices = np.where(mask)[0]\n\n    # Compute k-NN Graph (k=5)\n    k = 10\n    adj_matrix = kneighbors_graph(coords_wsi, k, mode=\"connectivity\", include_self=False)\n\n    edge_list = np.array(adj_matrix.nonzero())\n    edge_list[0] = node_indices[edge_list[0]]\n    edge_list[1] = node_indices[edge_list[1]]\n\n    edge_index_list.append(torch.tensor(edge_list, dtype=torch.long))\n\n# Merge all edges\nedge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long).contiguous()\n\n# Convert images to feature vectors (flatten each image)\n# x = torch.stack([torch.tensor(img, dtype=torch.float).flatten() for img in images])\n\nfeatures_test = []\nfor img in tqdm(images_test, desc=\"Extracting features\", total=len(images_test)):\n    features_test.append(extract_features(img))\n\nx = torch.stack(features_test)\n\n# Convert labels\nlabels = [np.array(label, dtype=np.float32) for label in labels] \ny = torch.tensor(np.vstack(labels), dtype=torch.float32)\n\n# Create Graph Data\ngraph_data_test = Data(x=x, edge_index=edge_index, y=y)\n\nprint(f\"Graph created with {graph_data_test.num_nodes} nodes and {graph_data_test.num_edges} edges.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class GNNModel(torch.nn.Module):\n#     def __init__(self, in_dim, hidden_dim, out_dim):\n#         super(GNNModel, self).__init__()\n#         self.conv1 = GCNConv(in_dim, hidden_dim)\n#         self.conv2 = GCNConv(hidden_dim, hidden_dim)\n#         self.fc = nn.Linear(hidden_dim, out_dim)\n\n#     def forward(self, x, edge_index):\n#         x = self.conv1(x, edge_index)\n#         x = self.conv2(x, edge_index)\n#         x = self.fc(x)\n#         return x\n\n\nclass GNNModel(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim):\n        super(GNNModel, self).__init__()\n        self.conv1 = GCNConv(in_dim, hidden_dim)\n        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n        self.fc = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x, edge_index):\n        x = F.relu(self.conv1(x, edge_index))\n        x = F.relu(self.conv2(x, edge_index))\n        x = self.fc(x)\n        return F.relu(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def spearman_score(y_true, y_pred):\n    \"\"\"Compute Spearman correlation per output dimension and return the mean.\"\"\"\n    y_true = y_true.cpu().detach().numpy()\n    y_pred = y_pred.cpu().detach().numpy()\n\n    scores = []\n    for i in range(y_true.shape[1]):  # Loop over each output dimension (35)\n        coef, _ = spearmanr(y_true[:, i], y_pred[:, i])\n        scores.append(coef if not np.isnan(coef) else 0.0)  # Handle NaNs\n\n    return np.mean(scores) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = GNNModel(in_dim=x.shape[1], hidden_dim=1024, out_dim=35).to(device)\n\n# Define Loss Function (MSE Loss for regression task)\ncriterion = nn.MSELoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n\n# Move Data to Device\ngraph_data = graph_data.to(device)\ngraph_data_test = graph_data_test.to(device)\n\n\n# Training Loop\nnum_epochs = 100000\nbest_loss = float(\"inf\")\nbest_model_path = \"best_model.pth\"\ncheckpoint_path = \"checkpoint.pth\"\n\nimport os\n\nif os.path.exists('/kaggle/working/best_mode.pth'):\n    print(\"Loading checkpoint...\")\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch']  # Resume from last epoch\n    print(f\"Resumed training from epoch {start_epoch}\")\nelse:\n    print(\"No checkpoint found. Starting from scratch.\")\n    start_epoch = 0\n\nfor epoch in range(start_epoch, num_epochs):  # Start from last saved epoch\n    total_spearman = 0\n    model.train()\n    optimizer.zero_grad()\n    \n    outputs = model(graph_data.x, graph_data.edge_index)\n    loss = criterion(outputs, graph_data.y)\n    loss.backward()\n    optimizer.step()\n# for epoch in range(num_epochs):\n#     total_spearman = 0\n#     model.train()\n#     optimizer.zero_grad()\n#     outputs = model(graph_data.x, graph_data.edge_index)\n#     loss = criterion(outputs, graph_data.y)\n#     loss.backward()\n#     optimizer.step()\n    spearman_corr = spearman_score(graph_data.y, outputs)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Spearman: {spearman_corr}\")\n\n    \n    # Save checkpoint\n    checkpoint = {\n        'epoch': epoch + 1,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }\n    if loss.item() < best_loss:\n        best_loss = loss.item()\n        torch.save(checkpoint, best_model_path)\n        print(f\"New best model saved with loss: {best_loss:.4f}\")\n        torch.save(checkpoint, checkpoint_path)\n        model.eval()\n        with torch.no_grad():\n            predictions = model(graph_data_test.x, graph_data_test.edge_index).cpu().numpy()\n            df_predictions = pd.DataFrame(predictions, columns=[f\"C{k}\" for k in range(1, 36)])\n            df_predictions['ID'] = df_predictions.index\n            df_predictions.to_csv(\"predictions.csv\", index=False)\nprint(\"Training complete.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}